#!/bin/bash
#SBATCH --job-name=libE_test
#SBATCH -N 4
#SBATCH -p bdwall #bdwall broadwell #knlall knl
#SBATCH -A COMPASSOPT
#SBATCH -o libE_test.%j.%N.out
#SBATCH -e libE_test.%j.%N.err
##SBATCH --ntasks-per-node=36
#SBATCH --time=02:00

export LIBE_PROCS=5
export EXE=test_6-hump_camel_with_different_nodes_uniform_sample.py

#Note use of tmi fabric may cause hang in mpi4py on intel systems (eg. Omnipath)
#export I_MPI_FABRICS=shm:tmi #test with "mpi4py.rc.recv_mprobe = False"
export I_MPI_FABRICS=shm:ofa

#Alt install in anaconda env - then wont need PYTHONPATH for this
export NLOPT_PYTHON_HOME="/home/shudson/install/intel/lib/python2.7/site-packages"
export PYTHONPATH="${PYTHONPATH}:${NLOPT_PYTHON_HOME}"
export MKL_DIR=/blues/gpfs/home/software/spack-0.10.1/opt/spack/linux-centos7-x86_64/intel-17.0.4/intel-mkl-2017.3.196-v7uuj6zmthzln35n2hb7i5u5ybncv5ev/mkl/lib/intel64_lin

#/bin/hostname

# PBS/Slurm eqiuv from https://www.chpc.utah.edu/documentation/software/slurm.php#m2svar

#Test
echo -e "Slurm job ID: $SLURM_JOBID"
 	
#cd $PBS_O_WORKDIR
cd $SLURM_SUBMIT_DIR

# A little useful information for the log file...
echo -e "Master process running on: $HOSTNAME"
echo -e "Directory is:  $PWD"

#Generate a listing of 1 node per line:
srun hostname | sort -u > nodefile.$SLURM_JOBID 
echo -e "\nSlurm has allocated the following nodes:"
cat nodefile.$SLURM_JOBID
NUM_NODES="$(wc -l < nodefile.$SLURM_JOBID)"
echo This job has allocated $NUM_NODES nodes

head -n 1 nodefile.$SLURM_JOBID > machinefile.$SLURM_JOBID
cat nodefile.$SLURM_JOBID | sort | uniq >> machinefile.$SLURM_JOBID
rm nodefile.$SLURM_JOBID

module load anaconda

#If want to check modules - they go to stderr by default
>&2 echo -e "\nListing loaded modules"
module list

#load my conda environment
. activate intel-libe-py2.7

#shudson LD_PRELOAD line - solves run-time error finding certain mkl libraries
export LD_PRELOAD=$MKL_DIR/libmkl_core.so:$MKL_DIR/libmkl_sequential.so


# Now run using either mpiexec or srun
# If no machine file supplied multinode jobs will default to launching node

#Using mpiexec/mpirun
#mpiexec -np $LIBE_PROCS -machinefile machinefile.$SLURM_JOBID python $EXE
#mpiexec -np $LIBE_PROCS -machinefile machinefile.$SLURM_JOBID python $EXE -m machinefile.$SLURM_JOBID

#To use srun
export SLURM_HOSTFILE=machinefile.$SLURM_JOBID
#srun -N4 -n $LIBE_PROCS -m arbitrary python $EXE
srun -N4 -n $LIBE_PROCS -m arbitrary python $EXE -m machinefile.$SLURM_JOBID


